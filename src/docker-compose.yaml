# ---
# x-airflow-common:
#   &airflow-common
#   # In order to add custom dependencies or upgrade provider distributions you can use your extended image.
#   # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml
#   # and uncomment the "build" line below, Then run `docker-compose build` to build the images.
#   # image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:3.1.3}
#   image: airflow-training:latest
#   build: ./airflow
#   env_file:
#     - ${ENV_FILE_PATH:-.env}
#   environment:
#     &airflow-common-env
#     AIRFLOW__CORE__EXECUTOR: CeleryExecutor
#     AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
#     AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
#     AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
#     AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
#     AIRFLOW__CORE__FERNET_KEY: ''
#     AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
#     AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
#     # AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://airflow-apiserver:8080/execution/'
#     AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://airflow-webserver:8080/api/v1/'
#     # yamllint disable rule:line-length
#     # Use simple http server on scheduler for health checks
#     # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server
#     # yamllint enable rule:line-length
#     AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
#     # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks
#     # for other purpose (development, test and especially production usage) build/extend Airflow image.
#     _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
#     # The following line can be used to set a custom config file, stored in the local config folder
#     AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'
#     MLFLOW_TRACKING_URI: 'http://mlflow-server:5500'
#     MLFLOW_S3_ENDPOINT_URL: 'http://minio:9000'
#     # KAFKA_BOOTSTRAP_SERVERS: 'kafka:9092'
#   volumes:
#     - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
#     - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
#     - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
#     - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
#     - ./models:/app/models
#     - ./config.yaml:/app/config.yaml
#     - ./src/data:/app/data
#     - ./.env:/app/.env
#   user: "${AIRFLOW_UID:-50000}:0"
#   depends_on:
#     &airflow-common-depends-on
#     redis:
#       condition: service_healthy
#     postgres:
#       condition: service_healthy

# services:
#   postgres:
#     image: postgres:15
#     environment:
#       POSTGRES_USER: airflow
#       POSTGRES_PASSWORD: airflow
#       POSTGRES_DB: airflow
#     volumes:
#       - postgres-db-volume:/var/lib/postgresql/data
#       - ./init-multiple-dbs.sh:/docker-entrypoint-initdb.d/init-multiple-dbs.sh
#     healthcheck:
#       test: ["CMD", "pg_isready", "-U", "airflow"]
#       interval: 10s
#       retries: 5
#       start_period: 5s
#     restart: always
#     networks:
#       - fraud-detection

#   redis:
#     # Redis is limited to 7.2-bookworm due to licencing change
#     # https://redis.io/blog/redis-adopts-dual-source-available-licensing/
#     image: redis:7.2-bookworm
#     expose:
#       - 6379
#     healthcheck:
#       test: ["CMD", "redis-cli", "ping"]
#       interval: 10s
#       timeout: 30s
#       retries: 50
#       start_period: 30s
#     restart: always
#     networks:
#       - fraud-detection

#   airflow-webserver:
#     <<: *airflow-common
#     command: webserver
#     ports:
#       - "8080:8080"
#     healthcheck:
#       test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 30s
#     restart: always
#     depends_on:
#       <<: *airflow-common-depends-on
#       airflow-init:
#         condition: service_completed_successfully
#     networks:
#       - fraud-detection

#   airflow-scheduler:
#     <<: *airflow-common
#     command: scheduler
#     healthcheck:
#       test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 30s
#     restart: always
#     depends_on:
#       <<: *airflow-common-depends-on
#       airflow-init:
#         condition: service_completed_successfully
#     networks:
#       - fraud-detection

#   airflow-dag-processor:
#     <<: *airflow-common
#     command: dag-processor
#     environment:
#           <<: *airflow-common-env
#           AIRFLOW__SCHEDULER__STANDALONE_DAG_PROCESSOR: "true"
#     healthcheck:
#       test: ["CMD-SHELL", 'airflow jobs check --job-type DagProcessorJob --hostname "$${HOSTNAME}"']
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 30s
#     restart: always
#     depends_on:
#       <<: *airflow-common-depends-on
#       airflow-init:
#         condition: service_completed_successfully
#     networks:
#       - fraud-detection

#   airflow-worker:
#     <<: *airflow-common
#     command: celery worker
#     healthcheck:
#       # yamllint disable rule:line-length
#       test:
#         - "CMD-SHELL"
#         - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 30s
#     environment:
#       <<: *airflow-common-env
#       # Required to handle warm shutdown of the celery workers properly
#       # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation
#       DUMB_INIT_SETSID: "0"
#     restart: always
#     depends_on:
#       <<: *airflow-common-depends-on
#       airflow-webserver:
#         condition: service_healthy
#       airflow-init:
#         condition: service_completed_successfully
#     deploy:
#       mode: replicated
#       replicas: 2
#     volumes:
#       - ./src/models:/app/src/models  
#       - ./src/data:/app/data          
#     networks:
#       - fraud-detection

#   airflow-triggerer:
#     <<: *airflow-common
#     command: triggerer
#     healthcheck:
#       test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 30s
#     restart: always
#     depends_on:
#       <<: *airflow-common-depends-on
#       airflow-init:
#         condition: service_completed_successfully
#     networks:
#       - fraud-detection

#   airflow-init:
#     <<: *airflow-common
#     entrypoint: /bin/bash
#     # yamllint disable rule:line-length
#     command:
#       - -c
#       - |
#         if [[ -z "${AIRFLOW_UID}" ]]; then
#           echo
#           echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
#           echo "If you are on Linux, you SHOULD follow the instructions below to set "
#           echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
#           echo "For other operating systems you can get rid of the warning with manually created .env file:"
#           echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
#           echo
#           export AIRFLOW_UID=$$(id -u)
#         fi
#         one_meg=1048576
#         mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
#         cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
#         disk_available=$$(df / | tail -1 | awk '{print $$4}')
#         warning_resources="false"
#         if (( mem_available < 4000 )) ; then
#           echo
#           echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
#           echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
#           echo
#           warning_resources="true"
#         fi
#         if (( cpus_available < 2 )); then
#           echo
#           echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
#           echo "At least 2 CPUs recommended. You have $${cpus_available}"
#           echo
#           warning_resources="true"
#         fi
#         if (( disk_available < one_meg * 10 )); then
#           echo
#           echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
#           echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
#           echo
#           warning_resources="true"
#         fi
#         if [[ $${warning_resources} == "true" ]]; then
#           echo
#           echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
#           echo "Please follow the instructions to increase amount of resources available:"
#           echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"
#           echo
#         fi
#         echo
#         echo "Creating missing opt dirs if missing:"
#         echo
#         mkdir -v -p /opt/airflow/{logs,dags,plugins,config}
#         echo
#         echo "Airflow version:"
#         /entrypoint airflow version
#         echo
#         echo "Files in shared volumes:"
#         echo
#         ls -la /opt/airflow/{logs,dags,plugins,config}
#         echo
#         echo "Running airflow config list to create default config file if missing."
#         echo
#         /entrypoint airflow config list >/dev/null
#         echo
#         echo "Files in shared volumes:"
#         echo
#         ls -la /opt/airflow/{logs,dags,plugins,config}
#         echo
#         echo "Change ownership of files in /opt/airflow to ${AIRFLOW_UID}:0"
#         echo
#         chown -R "${AIRFLOW_UID}:0" /opt/airflow/
#         echo
#         echo "Change ownership of files in shared volumes to ${AIRFLOW_UID}:0"
#         echo
#         chown -v -R "${AIRFLOW_UID}:0" /opt/airflow/{logs,dags,plugins,config}
#         echo
#         echo "Files in shared volumes:"
#         echo
#         ls -la /opt/airflow/{logs,dags,plugins,config}

#     # yamllint enable rule:line-length
#     environment:
#       <<: *airflow-common-env
#       _AIRFLOW_DB_MIGRATE: 'true'
#       _AIRFLOW_WWW_USER_CREATE: 'true'
#       _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
#       _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
#       _PIP_ADDITIONAL_REQUIREMENTS: ''
#     user: "0:0"
#     networks:
#       - fraud-detection

#   airflow-cli:
#     <<: *airflow-common
#     profiles:
#       - debug
#     environment:
#       <<: *airflow-common-env
#       CONNECTION_CHECK_MAX_COUNT: "0"
#     # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252
#     command:
#       - bash
#       - -c
#       - airflow
#     depends_on:
#       <<: *airflow-common-depends-on
#     networks:
#       - fraud-detection

#   # You can enable flower by adding "--profile flower" option e.g. docker-compose --profile flower up
#   # or by explicitly targeted on the command line e.g. docker-compose up flower.
#   # See: https://docs.docker.com/compose/profiles/
#   flower:
#     <<: *airflow-common
#     command: celery flower
#     profiles:
#       - flower
#     ports:
#       - "5555:5555"
#     healthcheck:
#       test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 30s
#     restart: always
#     depends_on:
#       <<: *airflow-common-depends-on
#       airflow-init:
#         condition: service_completed_successfully
#     networks:
#       - fraud-detection

#   # MINIO SERVER
#   mc:
#     image: minio/mc
#     platform: linux/amd64
#     depends_on:
#       - minio
#     container_name: mc
#     env_file:
#       - .env
#     entrypoint: >
#       /bin/sh -c "
#       /tmp/wait-for-it.sh minio:9000 &&
#       /usr/bin/mc alias set minio http://minio:9000 ${AWS_ACCESS_KEY_ID} ${AWS_SECRET_ACCESS_KEY} &&
#       /usr/bin/mc mb minio/mlflow;
#       exit 0;
#       "
#     volumes:
#       - ./wait-for-it.sh:/tmp/wait-for-it.sh
#     networks:
#       - fraud-detection

#   minio:
#     restart: always
#     image: minio/minio
#     platform: linux/amd64
#     container_name: minio
#     ports:
#       - "9000:9000"
#       - "9001:9001"
#     command: server /data --console-address ':9001' --address ':9000'
#     environment:
#       - MINIO_ROOT_USER=${MINIO_USERNAME}
#       - MINIO_ROOT_PASSWORD=${MINIO_PASSWORD}
#     volumes:
#       - minio_data:/data
#     networks:
#       - fraud-detection

#   # MLFLOW SERVER
#   mlflow-server:
#     restart: always
#     build: ./mlflow
#     image: mlflow-server
#     container_name: mlflow-server
#     depends_on:
#       - mc
#       - postgres
#     ports:
#       - "5500:5500"
#     environment:
#       - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
#       - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
#       - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
#     command: mlflow server --port 5500 --host 0.0.0.0 --backend-store-uri postgresql+psycopg2://mlflow:mlflow@postgres/mlflow --default-artifact-root s3://mlflow
#     networks:
#       - fraud-detection

#   # kafka producer
#   # producer:
#   #   build: ./producer
#   #   env_file: .env
#   #   volumes:
#   #     - ./aiven-certs/ca.pem:/app/ca.pem:ro
#   #     - ./aiven-certs/service.cert:/app/service.cert:ro
#   #     - ./aiven-certs/service.key:/app/service.key:ro
#   #   deploy:
#   #     replicas: 2
#   #     resources:
#   #       limits:
#   #         cpus: '1'
#   #         memory: 1G
#   #   networks:
#   #     - fraud-detection

#   # Producer
#   producer:
#     build: 
#       context: ..
#       dockerfile: src/producer/Dockerfile
#     env_file: .env
#     depends_on:
#       - kafka
#     deploy:
#       replicas: 2
#       resources:
#         limits:
#           cpus: '1'
#           memory: 1G
#     networks:
#       - fraud-detection

#   # Consumer
#   consumer:
#     build: 
#       context: ..
#       dockerfile: src/consumer/Dockerfile
#     env_file: .env
#     depends_on:
#       - kafka
#       - mlflow-server
#     deploy:
#       replicas: 1
#       resources:
#         limits:
#           cpus: '1'
#           memory: 1G
#     volumes:
#       - ./src/models:/app/src/models
#     networks:
#       - fraud-detection

# # # kafka consumer
# #   consumer:
# #     build: ./consumer
# #     env_file: .env
# #     deploy:
# #       replicas: 1  # Can scale up as needed
# #       resources:
# #         limits:
# #           cpus: '1'
# #           memory: 1G
# #     depends_on:
# #       - producer
# #     networks:
# #       - fraud-detection
# #     restart: unless-stopped

#   # Transaction Consumer (Simplified - no Aiven certs)
#   # consumer:
#   #   build: ./consumer
#   #   env_file: .env
#   #   depends_on:
#   #     - kafka
#   #     - ml_model
#   #   environment:
#   #     KAFKA_BOOTSTRAP_SERVERS: kafka:9092
#   #     KAFKA_TOPIC: transactions
#   #     KAFKA_CONSUMER_GROUP_ID: fraud-detection-group
#   #     KAFKA_SECURITY_PROTOCOL: PLAINTEXT
#   #     ML_MODEL_URL: http://ml_model:8000
#   #   volumes:
#   #     - ./consumer:/app
#   #   command: python main.py
#   #   networks:
#   #     - fraud-detection

#   # local kafka services
#   zookeeper:
#     image: confluentinc/cp-zookeeper:7.4.0
#     environment:
#       ZOOKEEPER_CLIENT_PORT: 2181
#       ZOOKEEPER_TICK_TIME: 2000
#     restart: always
#     # ports:
#     #   - "2181:2181"
#     networks:
#       - fraud-detection

#   kafka:
#     image: confluentinc/cp-kafka:7.4.0
#     depends_on:
#       - zookeeper
#     ports:
#       - "9092:9092"
#       # - "9093:9093"  
#     environment:
#         KAFKA_BROKER_ID: 1
#         KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
#         KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
#         KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
#         KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
#         KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
#         # KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9093
#         # KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
#         # KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
#         # KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
#         # KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
#         # KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
#         # KAFKA_LOG_RETENTION_HOURS: 168
#         # KAFKA_LOG_RETENTION_BYTES: -1
#     restart: always
#     networks:
#       - fraud-detection

#   kafka-ui:
#     image: provectuslabs/kafka-ui:latest
#     depends_on:
#       - kafka
#     ports:
#       - "8082:8080"
#     environment:
#       KAFKA_CLUSTERS_0_NAME: local
#       KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
#       KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
#       DYNAMIC_CONFIG_ENABLED: 'true'
#     networks:
#       - fraud-detection



#   # ML Model Service
#   # ml_model:
#   #   build: ./ml_model
#   #   ports:
#   #     - "8000:8000"
#   #   volumes:
#   #     - ./ml_model:/app
#   #   command: python -m uvicorn api:app --host 0.0.0.0 --port 8000
#   #   networks:
#   #     - fraud-detection

#     # Streamlit UI
  
#   # streamlit:
#   #   build: ./streamlit_app
#   #   ports:
#   #     - "8501:8501"
#   #   depends_on:
#   #     - ml_model
#   #     - postgres
#   #   environment:
#   #     ML_MODEL_URL: http://ml_model:8000
#   #     DATABASE_URL: postgresql://airflow:airflow@postgres/fraud_detection
#   #     KAFKA_BOOTSTRAP_SERVERS: kafka:9092
#   #   volumes:
#   #     - ./streamlit_app:/app
#   #   command: streamlit run app.py --server.port=8501 --server.address=0.0.0.0
#   #   networks:
#   #     - fraud-detection


# #   # Streamlit UI
#   streamlit:
#     build: 
#       context: ..
#       dockerfile: src/streamlit_app/Dockerfile
#     ports:
#       - "8501:8501"
#     environment:
#       PYTHONPATH: /app
#     volumes:
#       - ./src:/app/src
#       - ././data:/app/data
#       - ./src/models:/app/src/models
#     networks:
#       - fraud-detection
#     restart: unless-stopped


# volumes:
#   postgres-db-volume:
#   minio_data:

# networks:
#   fraud-detection:
#     driver: bridge






# ---
# x-airflow-common:
#   &airflow-common
#   image: airflow-training:latest
#   build: 
#     context: ..
#     dockerfile: src/airflow/Dockerfile
#   env_file:
#     - ${ENV_FILE_PATH:-.env}
#   environment:
#     &airflow-common-env
#     AIRFLOW__CORE__EXECUTOR: CeleryExecutor
#     AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
#     AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
#     AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
#     AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
#     AIRFLOW__CORE__FERNET_KEY: ''
#     AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
#     AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
#     AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://airflow-webserver:8080/api/v1/'
#     AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
#     _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
#     # AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'
#     MLFLOW_TRACKING_URI: 'http://mlflow-server:5500'
#     MLFLOW_S3_ENDPOINT_URL: 'http://minio:9000'
#   volumes:
#     - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
#     - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
#     # - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
#     - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
#     # ✅ FIX: Map local models folder to container path
#     - ./models:/app/src/models
#     # ✅ FIX: Map config from current dir
#     - ./config/config.yaml:/app/src/config/config.yaml
#     # ✅ FIX: Map data from Parent Directory (Project Root)
#     - ../data:/app/data
#     - ./.env:/app/.env
#   user: "${AIRFLOW_UID:-50000}:0"
#   depends_on:
#     &airflow-common-depends-on
#     redis:
#       condition: service_healthy
#     postgres:
#       condition: service_healthy

# services:
#   postgres:
#     image: postgres:15
#     environment:
#       POSTGRES_USER: airflow
#       POSTGRES_PASSWORD: airflow
#       POSTGRES_DB: airflow
#     volumes:
#       - postgres-db-volume:/var/lib/postgresql/data
#       - ./init-multiple-dbs.sh:/docker-entrypoint-initdb.d/init-multiple-dbs.sh
#     healthcheck:
#       test: ["CMD", "pg_isready", "-U", "airflow"]
#       interval: 10s
#       retries: 5
#       start_period: 5s
#     restart: always
#     networks:
#       - fraud-detection

#   redis:
#     image: redis:7.2-bookworm
#     expose:
#       - 6379
#     healthcheck:
#       test: ["CMD", "redis-cli", "ping"]
#       interval: 10s
#       timeout: 30s
#       retries: 50
#       start_period: 30s
#     restart: always
#     networks:
#       - fraud-detection

#   airflow-webserver:
#     <<: *airflow-common
#     command: webserver
#     ports:
#       - "8080:8080"
#     healthcheck:
#       test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 30s
#     restart: always
#     depends_on:
#       <<: *airflow-common-depends-on
#       airflow-init:
#         condition: service_completed_successfully
#     networks:
#       - fraud-detection

#   airflow-scheduler:
#     <<: *airflow-common
#     command: scheduler
#     healthcheck:
#       test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 30s
#     restart: always
#     depends_on:
#       <<: *airflow-common-depends-on
#       airflow-init:
#         condition: service_completed_successfully
#     environment:
#       AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT: 120
#       AIRFLOW__CORE__PARALLELISM: 2             
#       AIRFLOW__CORE__DAG_CONCURRENCY: 2          
#       AIRFLOW__CELERY__WORKER_CONCURRENCY: 2    
#     networks:
#       - fraud-detection

#   airflow-dag-processor:
#     <<: *airflow-common
#     command: dag-processor
#     environment:
#           <<: *airflow-common-env
#           AIRFLOW__SCHEDULER__STANDALONE_DAG_PROCESSOR: "true"
#           AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT: 120
#     healthcheck:
#       test: ["CMD-SHELL", 'airflow jobs check --job-type DagProcessorJob --hostname "$${HOSTNAME}"']
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 30s
#     restart: always
#     depends_on:
#       <<: *airflow-common-depends-on
#       airflow-init:
#         condition: service_completed_successfully
#     networks:
#       - fraud-detection

#   airflow-worker:
#     <<: *airflow-common
#     command: celery worker
#     healthcheck:
#       test:
#         - "CMD-SHELL"
#         - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 30s
#     environment:
#       <<: *airflow-common-env
#       DUMB_INIT_SETSID: "0"
#       AIRFLOW__CORE__PARALLELISM: 2             
#       AIRFLOW__CORE__DAG_CONCURRENCY: 2          
#       AIRFLOW__CELERY__WORKER_CONCURRENCY: 2
#     restart: always
#     depends_on:
#       <<: *airflow-common-depends-on
#       airflow-webserver:
#         condition: service_healthy
#       airflow-init:
#         condition: service_completed_successfully
#     deploy:
#       mode: replicated
#       replicas: 2
#     volumes:
#       # - ./models:/app/src/models   # Maps src/models to container
#       # - ../data:/app/data          # Maps Root/data to container
#       # - ./dags:/opt/airflow/dags   # Ensure DAGs are mapped
#       - ./dags:/opt/airflow/dags
#       - ./logs:/opt/airflow/logs
#       # - ./config:/opt/airflow/config
#       - ./plugins:/opt/airflow/plugins
#       # Existing mapping (Good for /app based paths
#       - ./models:/app/src/models
#       # Fixes the save location)
#       - ./models:/opt/airflow/src/models
#       - ./data:/opt/airflow/data
#       # # Just in case it looks for data relative to workdir
#       # - ../data:/opt/airflow/data
#     networks:
#       - fraud-detection

#   airflow-triggerer:
#     <<: *airflow-common
#     command: triggerer
#     healthcheck:
#       test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 30s
#     restart: always
#     depends_on:
#       <<: *airflow-common-depends-on
#       airflow-init:
#         condition: service_completed_successfully
#     networks:
#       - fraud-detection

#   airflow-init:
#     <<: *airflow-common
#     entrypoint: /bin/bash
#     command:
#       - -c
#       - |
#         if [[ -z "${AIRFLOW_UID}" ]]; then
#           echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
#           export AIRFLOW_UID=$$(id -u)
#         fi
#         mkdir -v -p /opt/airflow/{logs,dags,plugins,config}
#         /entrypoint airflow version
#         # /entrypoint airflow config list >/dev/null
#         chown -R "${AIRFLOW_UID}:0" /opt/airflow/
#         chown -v -R "${AIRFLOW_UID}:0" /opt/airflow/{logs,dags,plugins,config}
#     environment:
#       <<: *airflow-common-env
#       _AIRFLOW_DB_MIGRATE: 'true'
#       _AIRFLOW_WWW_USER_CREATE: 'true'
#       _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
#       _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
#       _PIP_ADDITIONAL_REQUIREMENTS: ''
#     user: "0:0"
#     networks:
#       - fraud-detection

#   airflow-cli:
#     <<: *airflow-common
#     profiles:
#       - debug
#     environment:
#       <<: *airflow-common-env
#       CONNECTION_CHECK_MAX_COUNT: "0"
#     command:
#       - bash
#       - -c
#       - airflow
#     depends_on:
#       <<: *airflow-common-depends-on
#     networks:
#       - fraud-detection

#   flower:
#     <<: *airflow-common
#     command: celery flower
#     profiles:
#       - flower
#     ports:
#       - "5555:5555"
#     healthcheck:
#       test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 30s
#     restart: always
#     depends_on:
#       <<: *airflow-common-depends-on
#       airflow-init:
#         condition: service_completed_successfully
#     networks:
#       - fraud-detection

#   mc:
#     image: minio/mc
#     platform: linux/amd64
#     depends_on:
#       - minio
#     container_name: mc
#     env_file:
#       - .env
#     entrypoint: >
#       /bin/sh -c "
#       /tmp/wait-for-it.sh minio:9000 &&
#       /usr/bin/mc alias set minio http://minio:9000 ${AWS_ACCESS_KEY_ID} ${AWS_SECRET_ACCESS_KEY} &&
#       /usr/bin/mc mb minio/mlflow;
#       exit 0;
#       "
#     volumes:
#       - ./wait-for-it.sh:/tmp/wait-for-it.sh
#     networks:
#       - fraud-detection

#   minio:
#     restart: always
#     image: minio/minio
#     platform: linux/amd64
#     container_name: minio
#     ports:
#       - "9000:9000"
#       - "9001:9001"
#     command: server /data --console-address ':9001' --address ':9000'
#     environment:
#       - MINIO_ROOT_USER=${MINIO_USERNAME}
#       - MINIO_ROOT_PASSWORD=${MINIO_PASSWORD}
#     volumes:
#       - minio_data:/data
#     networks:
#       - fraud-detection

#   mlflow-server:
#     restart: always
#     build: 
#       context: ..
#       dockerfile: src/mlflow/Dockerfile
#     image: mlflow-server
#     container_name: mlflow-server
#     depends_on:
#       - mc
#       - postgres
#     ports:
#       - "5500:5500"
#     environment:
#       - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
#       - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
#       - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
#     command: mlflow server --port 5500 --host 0.0.0.0 --backend-store-uri postgresql+psycopg2://mlflow:mlflow@postgres/mlflow --default-artifact-root s3://mlflow
#     networks:
#       - fraud-detection

#   producer:
#     build: 
#       context: ..
#       dockerfile: src/producer/Dockerfile
#     env_file: .env
#     depends_on:
#       - kafka
#     deploy:
#       replicas: 2
#       resources:
#         limits:
#           cpus: '1'
#           memory: 1G
#     networks:
#       - fraud-detection

#   consumer:
#     build: 
#       context: ..
#       dockerfile: src/consumer/Dockerfile
#     env_file: .env
#     depends_on:
#       - kafka
#       - mlflow-server
#     deploy:
#       replicas: 1
#       resources:
#         limits:
#           cpus: '1'
#           memory: 1G
#     # ✅ FIX: Map local src/models to container /app/src/models
#     volumes:
#       - ./models:/app/src/models
#     networks:
#       - fraud-detection

#   zookeeper:
#     image: confluentinc/cp-zookeeper:7.4.0
#     environment:
#       ZOOKEEPER_CLIENT_PORT: 2181
#       ZOOKEEPER_TICK_TIME: 2000
#     restart: always
#     networks:
#       - fraud-detection

#   kafka:
#     image: confluentinc/cp-kafka:7.4.0
#     depends_on:
#       - zookeeper
#     ports:
#       - "9092:9092"
#     environment:
#         KAFKA_BROKER_ID: 1
#         KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
#         KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
#         KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
#         KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
#         KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
#     restart: always
#     networks:
#       - fraud-detection

#   kafka-ui:
#     image: provectuslabs/kafka-ui:latest
#     depends_on:
#       - kafka
#     ports:
#       - "8082:8080"
#     environment:
#       KAFKA_CLUSTERS_0_NAME: local
#       KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
#       KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
#       DYNAMIC_CONFIG_ENABLED: 'true'
#     networks:
#       - fraud-detection

#   streamlit:
#     build: 
#       context: ..
#       dockerfile: src/streamlit_app/Dockerfile
#     ports:
#       - "8501:8501"
#     environment:
#       PYTHONPATH: /app
#     # ✅ FIX: ALL VOLUMES
#     volumes:
#       - .:/app/src                # Maps current src folder to /app/src
#       - ./data:/app/data         # Maps Parent data folder to /app/data
#       - ./models:/app/src/models  # Maps models to /app/src/models
#     networks:
#       - fraud-detection
#     restart: unless-stopped

# volumes:
#   postgres-db-volume:
#   minio_data:

# networks:
#   fraud-detection:
#     driver: bridge



x-airflow-common:
  &airflow-common
  image: airflow-training:latest
  build:
    context: ..
    dockerfile: src/airflow/Dockerfile
  env_file:
    - ${ENV_FILE_PATH:-.env}
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://airflow-webserver:8080/api/v1/'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
    MLFLOW_TRACKING_URI: 'http://mlflow-server:5500'
    MLFLOW_S3_ENDPOINT_URL: 'http://minio:9000'
    PYTHONPATH: '/opt/airflow/dags:/opt/airflow'
  volumes:
    # Standard Airflow directories
    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
    # Application data and models - single source of truth
    - ./models:/opt/airflow/src/models
    - ./data:/opt/airflow/data
    - ./config/config.yaml:/opt/airflow/config/config.yaml
    - ./.env:/opt/airflow/.env
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    &airflow-common-depends-on
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy

services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
      - ./init-multiple-dbs.sh:/docker-entrypoint-initdb.d/init-multiple-dbs.sh
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always
    networks:
      - fraud-detection

  redis:
    image: redis:7.2-bookworm
    expose:
      - 6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s
    restart: always
    networks:
      - fraud-detection

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    networks:
      - fraud-detection

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    environment:
      <<: *airflow-common-env
      AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT: 120
      AIRFLOW__CORE__PARALLELISM: 2
      AIRFLOW__CORE__DAG_CONCURRENCY: 2
      AIRFLOW__CELERY__WORKER_CONCURRENCY: 2
    networks:
      - fraud-detection

  airflow-dag-processor:
    <<: *airflow-common
    command: dag-processor
    environment:
      <<: *airflow-common-env
      AIRFLOW__SCHEDULER__STANDALONE_DAG_PROCESSOR: "true"
      AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT: 120
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type DagProcessorJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    networks:
      - fraud-detection

  airflow-worker:
    <<: *airflow-common
    command: celery worker
    healthcheck:
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    environment:
      <<: *airflow-common-env
      DUMB_INIT_SETSID: "0"
      AIRFLOW__CORE__PARALLELISM: 2
      AIRFLOW__CORE__DAG_CONCURRENCY: 2
      AIRFLOW__CELERY__WORKER_CONCURRENCY: 2
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-webserver:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    deploy:
      mode: replicated
      replicas: 2
    networks:
      - fraud-detection

  airflow-triggerer:
    <<: *airflow-common
    command: triggerer
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    networks:
      - fraud-detection

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        if [[ -z "${AIRFLOW_UID}" ]]; then
          echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
          export AIRFLOW_UID=$(id -u)
        fi
        mkdir -v -p /opt/airflow/{logs,dags,plugins,src/models,data}
        /entrypoint airflow version
        chown -R "${AIRFLOW_UID}:0" /opt/airflow/
        chown -v -R "${AIRFLOW_UID}:0" /opt/airflow/{logs,dags,plugins,src,data}
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
      _PIP_ADDITIONAL_REQUIREMENTS: ''
    user: "0:0"
    networks:
      - fraud-detection

  airflow-cli:
    <<: *airflow-common
    profiles:
      - debug
    environment:
      <<: *airflow-common-env
      CONNECTION_CHECK_MAX_COUNT: "0"
    command:
      - bash
      - -c
      - airflow
    depends_on:
      <<: *airflow-common-depends-on
    networks:
      - fraud-detection

  flower:
    <<: *airflow-common
    command: celery flower
    profiles:
      - flower
    ports:
      - "5555:5555"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    networks:
      - fraud-detection

  mc:
    image: minio/mc
    platform: linux/amd64
    depends_on:
      - minio
    container_name: mc
    env_file:
      - .env
    entrypoint: >
      /bin/sh -c "
      /tmp/wait-for-it.sh minio:9000 &&
      /usr/bin/mc alias set minio http://minio:9000 ${AWS_ACCESS_KEY_ID} ${AWS_SECRET_ACCESS_KEY} &&
      /usr/bin/mc mb minio/mlflow;
      exit 0;
      "
    volumes:
      - ./wait-for-it.sh:/tmp/wait-for-it.sh
    networks:
      - fraud-detection

  minio:
    restart: always
    image: minio/minio
    platform: linux/amd64
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    command: server /data --console-address ':9001' --address ':9000'
    environment:
      - MINIO_ROOT_USER=${MINIO_USERNAME}
      - MINIO_ROOT_PASSWORD=${MINIO_PASSWORD}
    volumes:
      - minio_data:/data
    networks:
      - fraud-detection

  mlflow-server:
    restart: always
    build:
      context: ..
      dockerfile: src/mlflow/Dockerfile
    image: mlflow-server
    container_name: mlflow-server
    depends_on:
      - mc
      - postgres
    ports:
      - "5500:5500"
    environment:
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    command: mlflow server --port 5500 --host 0.0.0.0 --backend-store-uri postgresql+psycopg2://mlflow:mlflow@postgres/mlflow --default-artifact-root s3://mlflow
    networks:
      - fraud-detection

  producer:
    build:
      context: ..
      dockerfile: src/producer/Dockerfile
    env_file: .env
    depends_on:
      - kafka
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '1'
          memory: 1G
    networks:
      - fraud-detection

  consumer:
    build:
      context: ..
      dockerfile: src/consumer/Dockerfile
    env_file: .env
    depends_on:
      - kafka
      - mlflow-server
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: '1'
          memory: 1G
    volumes:
      - ./models:/app/src/models
    networks:
      - fraud-detection

  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    restart: always
    networks:
      - fraud-detection

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    restart: always
    networks:
      - fraud-detection

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    depends_on:
      - kafka
    ports:
      - "8082:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
      DYNAMIC_CONFIG_ENABLED: 'true'
    networks:
      - fraud-detection

  streamlit:
    build:
      context: ..
      dockerfile: src/streamlit_app/Dockerfile
    ports:
      - "8501:8501"
    environment:
      PYTHONPATH: /app
    volumes:
      - .:/app/src
      - ./data:/app/data
      - ./models:/app/src/models
    networks:
      - fraud-detection
    restart: unless-stopped

volumes:
  postgres-db-volume:
  minio_data:

networks:
  fraud-detection:
    driver: bridge